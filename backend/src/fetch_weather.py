"""
fetch_weather.py

Thu tháº­p dá»¯ liá»‡u thá»i tiáº¿t 30 nÄƒm cho dá»± Ã¡n dá»± bÃ¡o nÄƒng suáº¥t cÃ  phÃª Äáº¯k Láº¯k.
Sá»­ dá»¥ng Open-Meteo Archive API.

Chá»©c nÄƒng:
- Káº¿t ná»‘i Open-Meteo Archive API
- Láº¥y dá»¯ liá»‡u nhiá»‡t Ä‘á»™, lÆ°á»£ng mÆ°a, Ä‘á»™ áº©m, soil moisture theo ngÃ y
- LÆ°u vÃ o thÆ° má»¥c data/external/
"""

import os
import requests
import pandas as pd
from datetime import datetime
import time

# ========================
# Cáº¤U HÃŒNH
# ========================
LATITUDE = 12.71
LONGITUDE = 108.23
START_DATE = "1990-01-01"
END_DATE = "2025-10-31"  # Cáº­p nháº­t Ä‘áº¿n thÃ¡ng 10/2025
TIMEZONE = "Asia/Bangkok"

# CÃ¡c biáº¿n khÃ­ háº­u DAILY
DAILY_VARIABLES = [
    "temperature_2m_max",
    "temperature_2m_min", 
    "precipitation_sum",
    "relative_humidity_2m_mean",
    "shortwave_radiation_sum"  # Bá»©c xáº¡ cho feature radiation_JunSep
]

# CÃ¡c biáº¿n HOURLY cáº§n aggregate (soil moisture khÃ´ng cÃ³ trong daily)
HOURLY_VARIABLES = [
    "soil_moisture_0_to_7cm",
    "soil_moisture_7_to_28cm"
]

# ÄÆ°á»ng dáº«n output
OUTPUT_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "external")
OUTPUT_FILE = os.path.join(OUTPUT_DIR, "weather_daklak_1990_2025.csv")  # Cáº­p nháº­t tÃªn file

# API endpoint
API_URL = "https://archive-api.open-meteo.com/v1/archive"


def fetch_weather_chunk(start_date: str, end_date: str, max_retries: int = 3) -> dict:
    """
    Fetch má»™t chunk dá»¯ liá»‡u thá»i tiáº¿t tá»« Open-Meteo Archive API.
    API cÃ³ giá»›i háº¡n, cáº§n chia nhá» request.
    """
    params = {
        "latitude": LATITUDE,
        "longitude": LONGITUDE,
        "start_date": start_date,
        "end_date": end_date,
        "daily": ",".join(DAILY_VARIABLES),
        "hourly": ",".join(HOURLY_VARIABLES),
        "timezone": TIMEZONE
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.get(API_URL, params=params, timeout=120)
            response.raise_for_status()
            return response.json()
            
        except requests.exceptions.RequestException as e:
            print(f"   âš ï¸ Lá»—i: {e}")
            if attempt < max_retries - 1:
                wait_time = (attempt + 1) * 5
                print(f"   â³ Äá»£i {wait_time} giÃ¢y...")
                time.sleep(wait_time)
            else:
                raise Exception(f"âŒ KhÃ´ng thá»ƒ fetch dá»¯ liá»‡u: {e}")
    
    return None


def fetch_weather_data_in_chunks():
    """
    Fetch dá»¯ liá»‡u theo tá»«ng nÄƒm Ä‘á»ƒ trÃ¡nh giá»›i háº¡n API.
    """
    all_daily_data = []
    all_hourly_data = []
    
    start_year = int(START_DATE[:4])
    end_year = int(END_DATE[:4])
    
    print(f"\nğŸ“… Sáº½ fetch dá»¯ liá»‡u tá»« {start_year} Ä‘áº¿n {end_year} ({end_year - start_year + 1} nÄƒm)")
    
    for year in range(start_year, end_year + 1):
        chunk_start = f"{year}-01-01"
        # Vá»›i nÄƒm cuá»‘i, sá»­ dá»¥ng END_DATE thay vÃ¬ 12-31
        if year == end_year:
            chunk_end = END_DATE
        else:
            chunk_end = f"{year}-12-31"
        
        print(f"   ğŸ“¡ NÄƒm {year}...", end=" ", flush=True)
        
        try:
            data = fetch_weather_chunk(chunk_start, chunk_end)
            
            # Láº¥y daily data
            if "daily" in data:
                daily_df = pd.DataFrame(data["daily"])
                all_daily_data.append(daily_df)
            
            # Láº¥y hourly data vÃ  aggregate theo ngÃ y
            if "hourly" in data:
                hourly_df = pd.DataFrame(data["hourly"])
                hourly_df["time"] = pd.to_datetime(hourly_df["time"])
                hourly_df["date"] = hourly_df["time"].dt.date
                
                # Aggregate hourly -> daily (mean)
                hourly_agg = hourly_df.groupby("date").agg({
                    "soil_moisture_0_to_7cm": "mean",
                    "soil_moisture_7_to_28cm": "mean"
                }).reset_index()
                hourly_agg["date"] = pd.to_datetime(hourly_agg["date"])
                all_hourly_data.append(hourly_agg)
            
            print("âœ…")
            
            # Delay nhá» giá»¯a cÃ¡c requests
            time.sleep(0.5)
            
        except Exception as e:
            print(f"âŒ {e}")
            continue
    
    return all_daily_data, all_hourly_data


def combine_and_process_data(daily_chunks, hourly_chunks):
    """
    GhÃ©p cÃ¡c chunks vÃ  táº¡o DataFrame hoÃ n chá»‰nh.
    """
    # GhÃ©p daily data
    daily_df = pd.concat(daily_chunks, ignore_index=True)
    daily_df["time"] = pd.to_datetime(daily_df["time"])
    daily_df = daily_df.rename(columns={"time": "date"})
    
    # GhÃ©p hourly aggregated data
    hourly_df = pd.concat(hourly_chunks, ignore_index=True)
    
    # Merge daily vÃ  hourly
    df = pd.merge(daily_df, hourly_df, on="date", how="left")
    
    # Äá»•i tÃªn cá»™t cho Ä‘áº¹p
    df = df.rename(columns={
        "temperature_2m_max": "temp_max",
        "temperature_2m_min": "temp_min",
        "precipitation_sum": "rain",
        "relative_humidity_2m_mean": "humidity",
        "shortwave_radiation_sum": "radiation",
        "soil_moisture_0_to_7cm": "soil_0_7",
        "soil_moisture_7_to_28cm": "soil_7_28"
    })
    
    # Sáº¯p xáº¿p theo ngÃ y
    df = df.sort_values("date").reset_index(drop=True)
    
    return df


def validate_data(df: pd.DataFrame) -> None:
    """
    Kiá»ƒm tra vÃ  cáº£nh bÃ¡o vá» dá»¯ liá»‡u.
    """
    print("\nğŸ“Š KIá»‚M TRA Dá»® LIá»†U:")
    print(f"   - Tá»•ng sá»‘ dÃ²ng: {len(df):,}")
    print(f"   - Khoáº£ng thá»i gian: {df['date'].min()} â†’ {df['date'].max()}")
    print(f"   - Sá»‘ nÄƒm: {df['date'].dt.year.nunique()}")
    
    # Kiá»ƒm tra missing values
    print("\nğŸ“‹ MISSING VALUES:")
    for col in df.columns:
        if col != "date":
            missing = df[col].isna().sum()
            pct = (missing / len(df)) * 100
            if missing > 0:
                print(f"   âš ï¸ {col}: {missing:,} ({pct:.2f}%)")
            else:
                print(f"   âœ… {col}: OK")
    
    # Cáº£nh bÃ¡o náº¿u Ã­t hÆ¡n 12,000 dÃ²ng
    if len(df) < 12000:
        print(f"\nâš ï¸ Cáº¢NH BÃO: Chá»‰ cÃ³ {len(df):,} dÃ²ng, Ã­t hÆ¡n 12,000 dÃ²ng yÃªu cáº§u!")
    else:
        print(f"\nâœ… Äá»§ dá»¯ liá»‡u: {len(df):,} dÃ²ng (> 12,000)")


def save_to_csv(df: pd.DataFrame, filepath: str) -> None:
    """
    LÆ°u DataFrame thÃ nh file CSV.
    """
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    df.to_csv(filepath, index=False)
    print(f"\nğŸ’¾ ÄÃ£ lÆ°u file: {filepath}")
    print(f"   - KÃ­ch thÆ°á»›c: {os.path.getsize(filepath) / 1024:.2f} KB")


def main():
    """Main function Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline."""
    print("=" * 60)
    print("ğŸŒ¦ FETCH Dá»® LIá»†U THá»œI TIáº¾T 30 NÄ‚M - Äáº®K Láº®K")
    print("=" * 60)
    print(f"\nğŸ“ Tá»a Ä‘á»™: {LATITUDE}Â°N, {LONGITUDE}Â°E")
    print(f"ğŸ“… Khoáº£ng thá»i gian: {START_DATE} â†’ {END_DATE}")
    print(f"ğŸŒ¡ Biáº¿n DAILY: {', '.join(DAILY_VARIABLES)}")
    print(f"ğŸ’§ Biáº¿n HOURLY (aggregate): {', '.join(HOURLY_VARIABLES)}")
    
    # Fetch dá»¯ liá»‡u theo chunks
    print("\n" + "-" * 60)
    daily_chunks, hourly_chunks = fetch_weather_data_in_chunks()
    
    # GhÃ©p vÃ  xá»­ lÃ½
    print("\nğŸ”„ Äang ghÃ©p vÃ  xá»­ lÃ½ dá»¯ liá»‡u...")
    df = combine_and_process_data(daily_chunks, hourly_chunks)
    
    # Validate
    validate_data(df)
    
    # LÆ°u file
    save_to_csv(df, OUTPUT_FILE)
    
    # Preview
    print("\nğŸ“‹ PREVIEW Dá»® LIá»†U:")
    print(df.head(10).to_string())
    print("\n...")
    print(df.tail(5).to_string())
    
    print("\n" + "=" * 60)
    print("âœ… ÄÃƒ Táº¢I XONG Dá»® LIá»†U 30 NÄ‚M â€” FILE CSV Sáº´N SÃ€NG CHO FEATURE ENGINEERING.")
    print("=" * 60)
    
    return df


if __name__ == "__main__":
    main()
